"""
Step 3: å¯¼å…¥ Chunks åˆ°æ•°æ®åº“

åŠŸèƒ½:
1. åˆ›å»ºæ•°æ®åº“è¡¨ç»“æ„
2. ä» data/processed/rag_chunks.json è¯»å–æ•°æ®
3. å¯¼å…¥åˆ° PostgreSQL æ•°æ®åº“

ä½¿ç”¨æ–¹æ³•:
    python3 pipeline/step3_import_to_database.py

æ•°æ®åº“è¿æ¥é…ç½®åœ¨ config/settings.py ä¸­è®¾ç½®
"""
import json
import sys
from pathlib import Path
import hashlib
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from datetime import datetime
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from database.rag_schema import Base, Staff, Publication, Chunk, create_tables
from config.settings import settings

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# é…ç½®
CONFIG = {
    "chunks_file": PROJECT_ROOT / "data/processed/rag_chunks.json",
}


def generate_publication_id(title: str, doi: str = None) -> str:
    """ç”Ÿæˆ publication ID"""
    if doi:
        return doi
    # æ—  DOI æ—¶ä½¿ç”¨ title çš„ hash
    return f"no-doi-{hashlib.md5(title.encode()).hexdigest()}"


def import_chunks_from_json(json_file: str, engine):
    """ä» JSON æ–‡ä»¶å¯¼å…¥ chunks åˆ°æ•°æ®åº“"""
    logger.info("="*80)
    logger.info("Step 3: å¯¼å…¥ Chunks åˆ°æ•°æ®åº“")
    logger.info("="*80)

    # è¯»å– JSON
    logger.info(f"\n1. è¯»å– JSON æ–‡ä»¶: {json_file}")
    with open(json_file, 'r') as f:
        chunks = json.load(f)

    logger.info(f"   âœ“ è¯»å–äº† {len(chunks)} ä¸ª chunks")

    # åˆ›å»º session
    Session = sessionmaker(bind=engine)
    session = Session()

    # ç»Ÿè®¡
    stats = {
        'staff_added': 0,
        'staff_updated': 0,
        'publications_added': 0,
        'publications_updated': 0,
        'chunks_added': 0,
        'chunks_skipped': 0,
    }

    # ç¼“å­˜ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
    processed_staff = set()
    processed_publications = set()

    logger.info(f"\n2. å¼€å§‹å¯¼å…¥...")

    try:
        for i, chunk in enumerate(chunks, 1):
            if i % 500 == 0:
                logger.info(f"   å¤„ç†è¿›åº¦: {i}/{len(chunks)} ({i/len(chunks)*100:.1f}%)")

            chunk_type = chunk.get('chunk_type')
            chunk_id = chunk.get('chunk_id')
            content = chunk.get('content')
            metadata = chunk.get('metadata', {})

            # æå– staff ä¿¡æ¯ (ä½¿ç”¨ profile_url ä½œä¸ºä¸»é”®)
            person_profile_url = metadata.get('person_profile_url') or metadata.get('profile_url')
            person_name = metadata.get('person_name')
            person_email = metadata.get('person_email')

            if not person_profile_url:
                stats['chunks_skipped'] += 1
                continue

            # 1. å¤„ç† Staff
            if person_profile_url not in processed_staff:
                existing_staff = session.query(Staff).filter_by(profile_url=person_profile_url).first()

                if not existing_staff:
                    staff = Staff(
                        profile_url=person_profile_url,
                        email=person_email,
                        full_name=person_name or 'Unknown',
                        role=metadata.get('role'),
                        school=metadata.get('person_school') or metadata.get('school'),
                        faculty=metadata.get('faculty'),
                    )
                    session.add(staff)
                    stats['staff_added'] += 1
                else:
                    stats['staff_updated'] += 1

                processed_staff.add(person_profile_url)

            # 2. å¤„ç† Publication
            publication_id = None
            if chunk_type in ['publication_title', 'publication_abstract', 'publication_keywords']:
                pub_title = metadata.get('pub_title')
                pub_doi = metadata.get('pub_doi')

                if pub_title:
                    publication_id = generate_publication_id(pub_title, pub_doi)

                    if publication_id not in processed_publications:
                        existing_pub = session.query(Publication).filter_by(id=publication_id).first()

                        if not existing_pub:
                            publication = Publication(
                                id=publication_id,
                                title=pub_title,
                                doi=pub_doi,
                                publication_year=metadata.get('pub_year'),
                                pub_type=metadata.get('pub_type'),
                                venue=metadata.get('pub_venue'),
                                abstract=content if chunk_type == 'publication_abstract' else None,
                                abstract_source=metadata.get('abstract_source', 'none'),
                                citations_count=metadata.get('citations_count', 0),
                                is_open_access=metadata.get('is_open_access', False),
                                has_doi=pub_doi is not None,
                                staff_profile_url=person_profile_url,
                                authors=[]
                            )
                            session.add(publication)
                            stats['publications_added'] += 1
                        else:
                            # æ›´æ–° abstract
                            if chunk_type == 'publication_abstract' and not existing_pub.abstract:
                                existing_pub.abstract = content
                                existing_pub.abstract_source = metadata.get('abstract_source', 'none')
                            stats['publications_updated'] += 1

                        processed_publications.add(publication_id)

            # 3. åˆ›å»º Chunk
            existing_chunk = session.query(Chunk).filter_by(chunk_id=chunk_id).first()

            if not existing_chunk:
                chunk_record = Chunk(
                    chunk_id=chunk_id,
                    chunk_type=chunk_type,
                    content=content,
                    chunk_metadata=metadata,
                    staff_profile_url=person_profile_url,
                    publication_id=publication_id
                )
                session.add(chunk_record)
                stats['chunks_added'] += 1
            else:
                stats['chunks_skipped'] += 1

            # å®šæœŸæäº¤
            if i % 1000 == 0:
                session.commit()
                logger.info(f"   ğŸ’¾ å·²æäº¤ {i} æ¡æ•°æ®")

        # æœ€ç»ˆæäº¤
        session.commit()
        logger.info(f"\n   âœ“ æœ€ç»ˆæäº¤å®Œæˆ")

    except Exception as e:
        logger.error(f"\n   âŒ é”™è¯¯: {e}")
        session.rollback()
        raise
    finally:
        session.close()

    # æ‰“å°ç»Ÿè®¡
    logger.info(f"\n3. å¯¼å…¥å®Œæˆ")
    logger.info("="*80)
    logger.info("ç»Ÿè®¡:")
    logger.info(f"  Staff:")
    logger.info(f"    æ–°å¢: {stats['staff_added']}")
    logger.info(f"    æ›´æ–°: {stats['staff_updated']}")
    logger.info(f"  Publications:")
    logger.info(f"    æ–°å¢: {stats['publications_added']}")
    logger.info(f"    æ›´æ–°: {stats['publications_updated']}")
    logger.info(f"  Chunks:")
    logger.info(f"    æ–°å¢: {stats['chunks_added']}")
    logger.info(f"    è·³è¿‡: {stats['chunks_skipped']}")
    logger.info("="*80)

    return stats


def main():
    """ä¸»å‡½æ•°"""
    chunks_file = CONFIG["chunks_file"]

    if not chunks_file.exists():
        logger.error(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {chunks_file}")
        logger.error(f"   è¯·å…ˆè¿è¡Œ step2_parse_publications.py ç”Ÿæˆ chunks")
        return

    # åˆ›å»ºæ•°æ®åº“å¼•æ“
    logger.info(f"è¿æ¥æ•°æ®åº“: {settings.postgres_dsn}")
    engine = create_engine(settings.postgres_dsn, echo=False)

    # åˆ›å»ºè¡¨
    logger.info("åˆ›å»ºæ•°æ®åº“è¡¨...")
    create_tables(engine)
    logger.info("âœ“ è¡¨åˆ›å»ºå®Œæˆ\n")

    # å¯¼å…¥æ•°æ®
    import_chunks_from_json(str(chunks_file), engine)

    logger.info(f"\nâœ“ æ•°æ®å¯¼å…¥å®Œæˆ!")


if __name__ == "__main__":
    main()
