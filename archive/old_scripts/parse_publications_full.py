"""
å®Œæ•´ç‰ˆ - è§£æUNSWå·¥ç¨‹å­¦é™¢staffæ•°æ®,æå–publicationså¹¶ä»OpenAlexè·å–è¯¦ç»†ä¿¡æ¯
ç‰¹æ€§:
- é”™è¯¯é‡è¯•æœºåˆ¶
- è¿›åº¦ä¿å­˜å’Œæ¢å¤
- è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯
- æŒ‰sectionsåˆ†å—(title, abstract, keywordsåˆ†å¼€)
- æ”¯æŒä¸­æ–­åç»§ç»­
"""
import json
import re
import requests
from time import sleep
from typing import List, Dict, Optional
import hashlib
import os
from datetime import datetime
from pathlib import Path

# é…ç½®
CONFIG = {
    "input_file": "/Users/z5241339/Documents/unsw_ai_rag/engineering_staff_with_profiles_cleaned.json",
    "output_file": "/Users/z5241339/Documents/unsw_ai_rag/rag_chunks_full.json",
    "progress_file": "/Users/z5241339/Documents/unsw_ai_rag/parsing_progress.json",
    "stats_file": "/Users/z5241339/Documents/unsw_ai_rag/parsing_statistics.json",
    "max_retries": 3,
    "retry_delay": 1.0,
    "api_delay": 0.15,  # OpenAlexæ¨è 100ms, æˆ‘ä»¬ç”¨150msæ›´ä¿é™©
    "email": "research@unsw.edu.au",
}

class PublicationParser:
    def __init__(self):
        self.stats = {
            "start_time": datetime.now().isoformat(),
            "total_staff": 0,
            "staff_with_publications": 0,
            "total_publications_parsed": 0,
            "publications_with_doi": 0,
            "openalex_success": 0,
            "openalex_not_found": 0,
            "openalex_errors": 0,
            "publications_with_abstract": 0,
            "publications_open_access": 0,
            "total_citations": 0,
            "chunks_created": 0,
            "errors": []
        }
        self.progress = self.load_progress()

    def load_progress(self) -> Dict:
        """åŠ è½½ä¹‹å‰çš„è¿›åº¦"""
        if os.path.exists(CONFIG["progress_file"]):
            with open(CONFIG["progress_file"], 'r') as f:
                return json.load(f)
        return {
            "processed_staff_emails": [],
            "openalex_cache": {}  # DOI -> OpenAlex data cache
        }

    def save_progress(self):
        """ä¿å­˜å½“å‰è¿›åº¦"""
        with open(CONFIG["progress_file"], 'w') as f:
            json.dump(self.progress, f, indent=2)

    def save_stats(self):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯"""
        self.stats["end_time"] = datetime.now().isoformat()
        with open(CONFIG["stats_file"], 'w') as f:
            json.dump(self.stats, f, indent=2, ensure_ascii=False)

    def parse_publication_text(self, pub_text: str, pub_type: str) -> List[Dict]:
        """è§£æpublicationæ–‡æœ¬å­—ç¬¦ä¸²,æå–individual publications"""
        publications = []
        pattern = rf'{re.escape(pub_type)} \| (\d{{4}})'

        try:
            entries = re.split(pattern, pub_text)

            for i in range(1, len(entries), 2):
                if i + 1 >= len(entries):
                    break

                year = entries[i]
                content = entries[i + 1].strip()

                # æå–DOI
                doi_match = re.search(r'http://dx\.doi\.org/([^\s,]+)', content)
                doi = doi_match.group(1) if doi_match else None

                # æå–æ ‡é¢˜ (åœ¨å•å¼•å·ä¹‹é—´)
                title_match = re.search(r"'([^']+)'", content)
                title = title_match.group(1) if title_match else None

                # æå–ä½œè€…
                authors_text = content.split(',')[0].strip() if ',' in content else ""

                if title or doi:
                    publications.append({
                        'year': int(year) if year.isdigit() else year,
                        'title': title,
                        'doi': doi,
                        'authors_text': authors_text,
                        'raw_text': content,
                        'pub_type': pub_type
                    })
        except Exception as e:
            self.stats["errors"].append(f"Parse error in {pub_type}: {str(e)}")

        return publications

    def invert_abstract_index(self, inverted_index: Dict) -> str:
        """å°†OpenAlexçš„å€’æ’ç´¢å¼•è½¬æ¢ä¸ºæ­£å¸¸æ–‡æœ¬"""
        if not inverted_index:
            return ""

        try:
            words = {}
            for word, positions in inverted_index.items():
                for pos in positions:
                    words[pos] = word
            return " ".join([words[i] for i in sorted(words.keys())])
        except Exception as e:
            self.stats["errors"].append(f"Abstract inversion error: {str(e)}")
            return ""

    def fetch_openalex(self, doi: str) -> Dict:
        """ä»OpenAlexè·å–å•ç¯‡è®ºæ–‡ä¿¡æ¯,å¸¦é‡è¯•æœºåˆ¶"""
        if not doi:
            return {"error": "no_doi"}

        # æ£€æŸ¥ç¼“å­˜
        if doi in self.progress["openalex_cache"]:
            return self.progress["openalex_cache"][doi]

        url = f"https://api.openalex.org/works/https://doi.org/{doi}"
        headers = {"User-Agent": f"mailto:{CONFIG['email']}"}

        for attempt in range(CONFIG["max_retries"]):
            try:
                response = requests.get(url, headers=headers, timeout=15)

                if response.status_code == 200:
                    data = response.json()
                    abstract = self.invert_abstract_index(data.get("abstract_inverted_index"))

                    result = {
                        "title": data.get("title"),
                        "abstract": abstract,
                        "publication_year": data.get("publication_year"),
                        "authors": [
                            {
                                "name": a.get("author", {}).get("display_name"),
                                "orcid": a.get("author", {}).get("orcid"),
                            }
                            for a in data.get("authorships", [])
                        ],
                        "venue": data.get("primary_location", {}).get("source", {}).get("display_name"),
                        "citations_count": data.get("cited_by_count", 0),
                        "is_open_access": data.get("open_access", {}).get("is_oa", False),
                        "pdf_url": data.get("open_access", {}).get("oa_url"),
                        "concepts": [
                            {
                                "name": c.get("display_name"),
                                "score": c.get("score", 0),
                                "level": c.get("level", 0)
                            }
                            for c in data.get("concepts", [])[:20]
                        ],
                        "type": data.get("type"),
                    }

                    # ç¼“å­˜ç»“æœ
                    self.progress["openalex_cache"][doi] = result
                    self.stats["openalex_success"] += 1

                    if abstract:
                        self.stats["publications_with_abstract"] += 1
                    if result["is_open_access"]:
                        self.stats["publications_open_access"] += 1
                    self.stats["total_citations"] += result["citations_count"]

                    return result

                elif response.status_code == 404:
                    result = {"error": "not_found"}
                    self.progress["openalex_cache"][doi] = result
                    self.stats["openalex_not_found"] += 1
                    return result
                else:
                    if attempt < CONFIG["max_retries"] - 1:
                        sleep(CONFIG["retry_delay"] * (attempt + 1))
                        continue
                    result = {"error": f"status_{response.status_code}"}
                    self.stats["openalex_errors"] += 1
                    return result

            except Exception as e:
                if attempt < CONFIG["max_retries"] - 1:
                    sleep(CONFIG["retry_delay"] * (attempt + 1))
                    continue
                result = {"error": str(e)}
                self.stats["openalex_errors"] += 1
                self.stats["errors"].append(f"OpenAlex fetch error for {doi}: {str(e)}")
                return result

        return {"error": "max_retries_exceeded"}

    def create_rag_chunks(self, staff_entry: Dict, publication_data: List[Dict]) -> List[Dict]:
        """
        ä¸ºæ¯ä¸ªç ”ç©¶äººå‘˜åˆ›å»ºå¤šä¸ªRAG chunks
        ç­–ç•¥: æ¯ç¯‡è®ºæ–‡åˆ†æˆå¤šä¸ªchunksä»¥æé«˜æ£€ç´¢ç²¾åº¦
        """
        chunks = []

        # Chunk Type 1: äººå‘˜åŸºæœ¬ä¿¡æ¯
        person_basic_chunk = {
            "chunk_id": f"person_basic_{staff_entry['email']}",
            "chunk_type": "person_basic",
            "content": f"{staff_entry['full_name']}\n"
                       f"Position: {staff_entry['role']}\n"
                       f"School: {staff_entry['school']}\n"
                       f"Faculty: {staff_entry['faculty']}\n"
                       f"Email: {staff_entry['email']}",
            "metadata": {
                "person_name": staff_entry['full_name'],
                "person_email": staff_entry['email'],
                "role": staff_entry['role'],
                "school": staff_entry['school'],
                "faculty": staff_entry['faculty'],
                "profile_url": staff_entry['profile_url'],
            }
        }
        chunks.append(person_basic_chunk)

        # Chunk Type 2: äººå‘˜ç ”ç©¶ä»‹ç»
        if staff_entry.get('biography'):
            person_bio_chunk = {
                "chunk_id": f"person_bio_{staff_entry['email']}",
                "chunk_type": "person_biography",
                "content": f"{staff_entry['full_name']} - Biography and Research Interests\n\n"
                           f"{staff_entry['biography']}\n\n"
                           f"Research Areas: {staff_entry.get('research_text', '')}",
                "metadata": {
                    "person_name": staff_entry['full_name'],
                    "person_email": staff_entry['email'],
                    "school": staff_entry['school'],
                    "faculty": staff_entry['faculty'],
                    "profile_url": staff_entry['profile_url'],
                }
            }
            chunks.append(person_bio_chunk)

        # Chunk Type 3-N: è®ºæ–‡chunks (æ¯ç¯‡è®ºæ–‡å¯èƒ½äº§ç”Ÿå¤šä¸ªchunks)
        for pub in publication_data:
            oa_data = pub.get('openalex_data', {})

            if 'error' in oa_data:
                # å³ä½¿æ²¡æœ‰OpenAlexæ•°æ®,ä¹Ÿåˆ›å»ºåŸºæœ¬chunk
                if pub.get('title'):
                    basic_pub_chunk = self._create_basic_publication_chunk(staff_entry, pub)
                    if basic_pub_chunk:
                        chunks.append(basic_pub_chunk)
                continue

            # ä¸ºæœ‰OpenAlexæ•°æ®çš„è®ºæ–‡åˆ›å»ºè¯¦ç»†chunks
            pub_chunks = self._create_detailed_publication_chunks(staff_entry, pub, oa_data)
            chunks.extend(pub_chunks)

        self.stats["chunks_created"] += len(chunks)
        return chunks

    def _create_basic_publication_chunk(self, staff: Dict, pub: Dict) -> Optional[Dict]:
        """ä¸ºæ²¡æœ‰OpenAlexæ•°æ®çš„è®ºæ–‡åˆ›å»ºåŸºæœ¬chunk"""
        title = pub.get('title')
        if not title:
            return None

        pub_id = pub.get('doi') or hashlib.md5(title.encode()).hexdigest()

        return {
            "chunk_id": f"pub_basic_{pub_id}",
            "chunk_type": "publication_basic",
            "content": f"Title: {title}\n"
                       f"Author: {staff['full_name']}\n"
                       f"Year: {pub.get('year')}\n"
                       f"Type: {pub.get('pub_type')}",
            "metadata": {
                "person_name": staff['full_name'],
                "person_email": staff['email'],
                "person_profile_url": staff['profile_url'],
                "person_school": staff['school'],
                "pub_title": title,
                "pub_year": pub.get('year'),
                "pub_type": pub.get('pub_type'),
                "pub_doi": pub.get('doi'),
                "has_abstract": False,
            }
        }

    def _create_detailed_publication_chunks(self, staff: Dict, pub: Dict, oa_data: Dict) -> List[Dict]:
        """ä¸ºæœ‰å®Œæ•´OpenAlexæ•°æ®çš„è®ºæ–‡åˆ›å»ºå¤šä¸ªç»†ç²’åº¦chunks"""
        chunks = []

        title = oa_data.get('title') or pub.get('title', 'Unknown Title')
        pub_id = pub.get('doi') or hashlib.md5(title.encode()).hexdigest()
        year = oa_data.get('publication_year') or pub.get('year')
        venue = oa_data.get('venue')
        abstract = oa_data.get('abstract', '')
        concepts = oa_data.get('concepts', [])

        # å…±äº«çš„metadata
        base_metadata = {
            "person_name": staff['full_name'],
            "person_email": staff['email'],
            "person_profile_url": staff['profile_url'],
            "person_school": staff['school'],
            "person_faculty": staff['faculty'],
            "pub_title": title,
            "pub_year": year,
            "pub_type": pub.get('pub_type'),
            "pub_doi": pub.get('doi'),
            "pub_venue": venue,
            "citations_count": oa_data.get('citations_count', 0),
            "is_open_access": oa_data.get('is_open_access', False),
            "pdf_url": oa_data.get('pdf_url'),
        }

        # Chunk 3a: è®ºæ–‡æ ‡é¢˜å’Œå…ƒæ•°æ® (ç”¨äºç²¾ç¡®åŒ¹é…è®ºæ–‡æŸ¥è¯¢)
        authors = oa_data.get('authors', [])
        author_names = [a['name'] for a in authors if a.get('name')]

        title_chunk = {
            "chunk_id": f"pub_title_{pub_id}",
            "chunk_type": "publication_title",
            "content": f"Title: {title}\n"
                       f"Authors: {', '.join(author_names[:10])}\n"  # é™åˆ¶ä½œè€…æ•°é‡
                       f"Published in: {venue} ({year})\n"
                       f"Type: {pub.get('pub_type')}\n"
                       f"Citations: {oa_data.get('citations_count', 0)}",
            "metadata": {**base_metadata, "has_abstract": bool(abstract)}
        }
        chunks.append(title_chunk)

        # Chunk 3b: è®ºæ–‡æ‘˜è¦ (å¦‚æœæœ‰) - è¿™æ˜¯æœ€é‡è¦çš„å†…å®¹chunk
        if abstract:
            abstract_chunk = {
                "chunk_id": f"pub_abstract_{pub_id}",
                "chunk_type": "publication_abstract",
                "content": f"Paper: {title}\n"
                           f"Author: {staff['full_name']} ({staff['school']})\n"
                           f"Year: {year}\n\n"
                           f"Abstract:\n{abstract}",
                "metadata": {**base_metadata, "has_abstract": True}
            }
            chunks.append(abstract_chunk)

        # Chunk 3c: è®ºæ–‡å…³é”®è¯/æ¦‚å¿µ (ç”¨äºä¸»é¢˜æ£€ç´¢)
        if concepts:
            high_score_concepts = [c['name'] for c in concepts if c.get('score', 0) > 0.3]
            if high_score_concepts:
                keywords_chunk = {
                    "chunk_id": f"pub_keywords_{pub_id}",
                    "chunk_type": "publication_keywords",
                    "content": f"Paper: {title}\n"
                               f"Author: {staff['full_name']}\n"
                               f"Keywords: {', '.join(high_score_concepts)}\n"
                               f"Research Topics: {', '.join(high_score_concepts[:5])}",
                    "metadata": {
                        **base_metadata,
                        "keywords": high_score_concepts,
                        "has_abstract": bool(abstract)
                    }
                }
                chunks.append(keywords_chunk)

        return chunks

    def process_staff(self, staff: Dict) -> List[Dict]:
        """å¤„ç†å•ä¸ªstaffæˆå‘˜"""
        email = staff['email']

        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
        if email in self.progress["processed_staff_emails"]:
            print(f"  â­  Skipping {staff['full_name']} (already processed)")
            return []

        if not staff.get('profile_details') or not staff['profile_details'].get('publications'):
            self.progress["processed_staff_emails"].append(email)
            return []

        pubs = staff['profile_details']['publications']
        staff_pubs = []

        # è§£æpublications
        for pub_type, pub_text in pubs.items():
            parsed = self.parse_publication_text(pub_text, pub_type)
            staff_pubs.extend(parsed)

        if not staff_pubs:
            self.progress["processed_staff_emails"].append(email)
            return []

        self.stats["staff_with_publications"] += 1
        self.stats["total_publications_parsed"] += len(staff_pubs)

        # è·å–OpenAlexæ•°æ®
        for pub in staff_pubs:
            if pub.get('doi'):
                self.stats["publications_with_doi"] += 1
                pub['openalex_data'] = self.fetch_openalex(pub['doi'])
                sleep(CONFIG["api_delay"])  # ç¤¼è²Œæ€§å»¶è¿Ÿ
            else:
                pub['openalex_data'] = {"error": "no_doi"}

        # åˆ›å»ºchunks
        chunks = self.create_rag_chunks(staff, staff_pubs)

        # æ ‡è®°ä¸ºå·²å¤„ç†
        self.progress["processed_staff_emails"].append(email)

        return chunks

    def run(self):
        """è¿è¡Œå®Œæ•´çš„è§£ææµç¨‹"""
        print("="*80)
        print("UNSW Engineering Staff Publications Parser - Full Version")
        print("="*80)

        # åŠ è½½æ•°æ®
        print("\n[1/4] Loading staff data...")
        with open(CONFIG["input_file"], 'r') as f:
            staff_data = json.load(f)

        self.stats["total_staff"] = len(staff_data)
        already_processed = len(self.progress["processed_staff_emails"])

        print(f"âœ“ Total staff: {len(staff_data)}")
        if already_processed > 0:
            print(f"âœ“ Already processed: {already_processed}")
            print(f"âœ“ Remaining: {len(staff_data) - already_processed}")

        # å¤„ç†æ¯ä¸ªstaff
        print(f"\n[2/4] Processing staff members...")
        all_chunks = []

        try:
            for i, staff in enumerate(staff_data, 1):
                if staff['email'] in self.progress["processed_staff_emails"]:
                    continue

                print(f"\n[{i}/{len(staff_data)}] {staff['full_name']}")
                print(f"  School: {staff['school']}")

                chunks = self.process_staff(staff)
                all_chunks.extend(chunks)

                if chunks:
                    print(f"  âœ“ Created {len(chunks)} chunks")

                # æ¯10ä¸ªstaffä¿å­˜ä¸€æ¬¡è¿›åº¦
                if i % 10 == 0:
                    self.save_progress()
                    print(f"\n  ğŸ’¾ Progress saved (processed {len(self.progress['processed_staff_emails'])}/{len(staff_data)})")

        except KeyboardInterrupt:
            print("\n\nâš ï¸  Interrupted by user. Saving progress...")
            self.save_progress()
            self.save_stats()
            print("âœ“ Progress saved. You can resume later.")
            return

        # ä¿å­˜æœ€ç»ˆç»“æœ
        print(f"\n[3/4] Saving chunks...")
        with open(CONFIG["output_file"], 'w') as f:
            json.dump(all_chunks, f, indent=2, ensure_ascii=False)
        print(f"âœ“ Saved {len(all_chunks)} chunks to {CONFIG['output_file']}")

        # ä¿å­˜ç»Ÿè®¡
        print(f"\n[4/4] Saving statistics...")
        self.save_progress()
        self.save_stats()
        print(f"âœ“ Statistics saved to {CONFIG['stats_file']}")

        # æ‰“å°ç»Ÿè®¡
        self._print_statistics()

    def _print_statistics(self):
        """æ‰“å°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*80)
        print("FINAL STATISTICS")
        print("="*80)

        print(f"\nğŸ“Š Staff Statistics:")
        print(f"  Total staff: {self.stats['total_staff']}")
        print(f"  Staff with publications: {self.stats['staff_with_publications']}")

        print(f"\nğŸ“š Publication Statistics:")
        print(f"  Total publications parsed: {self.stats['total_publications_parsed']}")
        print(f"  Publications with DOI: {self.stats['publications_with_doi']}")

        print(f"\nğŸŒ OpenAlex Fetch Results:")
        print(f"  Successful: {self.stats['openalex_success']}")
        print(f"  Not found: {self.stats['openalex_not_found']}")
        print(f"  Errors: {self.stats['openalex_errors']}")

        if self.stats['openalex_success'] > 0:
            abstract_rate = self.stats['publications_with_abstract'] / self.stats['openalex_success'] * 100
            oa_rate = self.stats['publications_open_access'] / self.stats['openalex_success'] * 100
            avg_citations = self.stats['total_citations'] / self.stats['openalex_success']

            print(f"\nğŸ“„ Content Quality:")
            print(f"  Publications with abstract: {self.stats['publications_with_abstract']} ({abstract_rate:.1f}%)")
            print(f"  Open access publications: {self.stats['publications_open_access']} ({oa_rate:.1f}%)")
            print(f"  Average citations: {avg_citations:.1f}")

        print(f"\nğŸ“¦ Chunks Created:")
        print(f"  Total chunks: {self.stats['chunks_created']}")

        if self.stats['errors']:
            print(f"\nâš ï¸  Errors encountered: {len(self.stats['errors'])}")
            print(f"  (See {CONFIG['stats_file']} for details)")

if __name__ == "__main__":
    parser = PublicationParser()
    parser.run()
