"""
å°† V2 ç”Ÿæˆçš„ chunks JSON å¯¼å…¥åˆ°æ•°æ®åº“

ä½¿ç”¨æ–¹æ³•:
    python3 scripts/import_chunks_to_db.py
"""
import json
import sys
from pathlib import Path
import hashlib
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from database.rag_schema import Base, Staff, Publication, Chunk, create_tables
from config.settings import settings


def generate_publication_id(title: str, doi: str = None) -> str:
    """ç”Ÿæˆ publication ID"""
    if doi:
        return doi
    # æ—  DOI æ—¶ä½¿ç”¨ title çš„ hash
    return f"no-doi-{hashlib.md5(title.encode()).hexdigest()}"


def import_chunks_from_json(json_file: str, engine):
    """
    ä» JSON æ–‡ä»¶å¯¼å…¥ chunks åˆ°æ•°æ®åº“

    Args:
        json_file: chunks JSON æ–‡ä»¶è·¯å¾„
        engine: SQLAlchemy engine
    """
    print("="*80)
    print("å¯¼å…¥ Chunks åˆ°æ•°æ®åº“")
    print("="*80)

    # è¯»å– JSON
    print(f"\n1. è¯»å– JSON æ–‡ä»¶: {json_file}")
    with open(json_file, 'r') as f:
        chunks = json.load(f)

    print(f"   âœ“ è¯»å–äº† {len(chunks)} ä¸ª chunks")

    # åˆ›å»º session
    Session = sessionmaker(bind=engine)
    session = Session()

    # ç»Ÿè®¡
    stats = {
        'staff_added': 0,
        'staff_updated': 0,
        'publications_added': 0,
        'publications_updated': 0,
        'chunks_added': 0,
        'chunks_skipped': 0,
    }

    # ç¼“å­˜ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
    processed_staff = set()
    processed_publications = set()

    print(f"\n2. å¼€å§‹å¯¼å…¥...")

    try:
        for i, chunk in enumerate(chunks, 1):
            if i % 500 == 0:
                print(f"   å¤„ç†è¿›åº¦: {i}/{len(chunks)} ({i/len(chunks)*100:.1f}%)")

            chunk_type = chunk.get('chunk_type')
            chunk_id = chunk.get('chunk_id')
            content = chunk.get('content')
            metadata = chunk.get('metadata', {})

            # æå– staff ä¿¡æ¯
            person_email = metadata.get('person_email')
            person_name = metadata.get('person_name')

            if not person_email:
                stats['chunks_skipped'] += 1
                continue

            # 1. å¤„ç† Staff
            if person_email not in processed_staff:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing_staff = session.query(Staff).filter_by(email=person_email).first()

                if not existing_staff:
                    # åˆ›å»ºæ–° staff
                    staff = Staff(
                        email=person_email,
                        full_name=person_name or 'Unknown',
                        role=metadata.get('role'),
                        school=metadata.get('person_school') or metadata.get('school'),
                        faculty=metadata.get('faculty'),
                        profile_url=metadata.get('person_profile_url') or metadata.get('profile_url'),
                    )
                    session.add(staff)
                    stats['staff_added'] += 1
                else:
                    stats['staff_updated'] += 1

                processed_staff.add(person_email)

            # 2. å¤„ç† Publication (å¦‚æœæ˜¯ publication chunk)
            publication_id = None
            if chunk_type in ['publication_title', 'publication_abstract', 'publication_keywords']:
                pub_title = metadata.get('pub_title')
                pub_doi = metadata.get('pub_doi')

                if pub_title:
                    publication_id = generate_publication_id(pub_title, pub_doi)

                    if publication_id not in processed_publications:
                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                        existing_pub = session.query(Publication).filter_by(id=publication_id).first()

                        if not existing_pub:
                            # åˆ›å»ºæ–° publication
                            publication = Publication(
                                id=publication_id,
                                title=pub_title,
                                doi=pub_doi,
                                publication_year=metadata.get('pub_year'),
                                pub_type=metadata.get('pub_type'),
                                venue=metadata.get('pub_venue'),
                                abstract=content if chunk_type == 'publication_abstract' else None,
                                abstract_source=metadata.get('abstract_source', 'none'),
                                citations_count=metadata.get('citations_count', 0),
                                is_open_access=metadata.get('is_open_access', False),
                                has_doi=pub_doi is not None,
                                staff_email=person_email,
                                authors=[]  # å¯ä»¥ä» chunk content è§£æ
                            )
                            session.add(publication)
                            stats['publications_added'] += 1
                        else:
                            # æ›´æ–° abstractï¼ˆå¦‚æœè¿™æ˜¯ abstract chunkï¼‰
                            if chunk_type == 'publication_abstract' and not existing_pub.abstract:
                                existing_pub.abstract = content
                                existing_pub.abstract_source = metadata.get('abstract_source', 'none')
                            stats['publications_updated'] += 1

                        processed_publications.add(publication_id)

            # 3. åˆ›å»º Chunk
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing_chunk = session.query(Chunk).filter_by(chunk_id=chunk_id).first()

            if not existing_chunk:
                chunk_record = Chunk(
                    chunk_id=chunk_id,
                    chunk_type=chunk_type,
                    content=content,
                    chunk_metadata=metadata,  # æ³¨æ„ï¼šä½¿ç”¨ chunk_metadata è€Œä¸æ˜¯ metadata
                    staff_email=person_email,
                    publication_id=publication_id
                )
                session.add(chunk_record)
                stats['chunks_added'] += 1
            else:
                stats['chunks_skipped'] += 1

            # å®šæœŸæäº¤ï¼ˆæ¯ 1000 æ¡ï¼‰
            if i % 1000 == 0:
                session.commit()
                print(f"   ğŸ’¾ å·²æäº¤ {i} æ¡æ•°æ®")

        # æœ€ç»ˆæäº¤
        session.commit()
        print(f"\n   âœ“ æœ€ç»ˆæäº¤å®Œæˆ")

    except Exception as e:
        print(f"\n   âŒ é”™è¯¯: {e}")
        session.rollback()
        raise
    finally:
        session.close()

    # æ‰“å°ç»Ÿè®¡
    print(f"\n3. å¯¼å…¥å®Œæˆ")
    print("="*80)
    print("ç»Ÿè®¡:")
    print(f"  Staff:")
    print(f"    æ–°å¢: {stats['staff_added']}")
    print(f"    æ›´æ–°: {stats['staff_updated']}")
    print(f"  Publications:")
    print(f"    æ–°å¢: {stats['publications_added']}")
    print(f"    æ›´æ–°: {stats['publications_updated']}")
    print(f"  Chunks:")
    print(f"    æ–°å¢: {stats['chunks_added']}")
    print(f"    è·³è¿‡: {stats['chunks_skipped']}")
    print("="*80)

    return stats


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    chunks_file = project_root / "rag_chunks_multisource_v2.json"

    if not chunks_file.exists():
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {chunks_file}")
        print(f"   è¯·å…ˆè¿è¡Œ parse_publications_multisource_v2.py ç”Ÿæˆ chunks")
        return

    # åˆ›å»ºæ•°æ®åº“å¼•æ“
    print(f"è¿æ¥æ•°æ®åº“: {settings.postgres_dsn}")
    engine = create_engine(settings.postgres_dsn, echo=False)

    # åˆ›å»ºè¡¨
    print("åˆ›å»ºæ•°æ®åº“è¡¨...")
    create_tables(engine)
    print("âœ“ è¡¨åˆ›å»ºå®Œæˆ")

    # å¯¼å…¥æ•°æ®
    import_chunks_from_json(str(chunks_file), engine)


if __name__ == "__main__":
    main()
